{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoCellLabeler on Freely-Moving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import nrrd\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import shutil\n",
    "\n",
    "import openpyxl\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import itertools\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from autolabel import *\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets, neuron names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_prj_neuropal = [\"2022-07-15-06\", \"2022-07-15-12\", \"2022-07-20-01\", \"2022-07-26-01\", \"2022-08-02-01\", \"2023-01-23-08\", \"2023-01-23-15\", \"2023-01-23-21\", \"2023-01-19-08\", \"2023-01-19-22\", \"2023-01-09-28\", \"2023-01-17-01\", \"2023-01-19-15\", \"2023-01-23-01\", \"2023-03-07-01\", \"2022-12-21-06\", \"2023-01-05-18\", \"2023-01-06-01\", \"2023-01-06-08\", \"2023-01-09-08\", \"2023-01-09-15\", \"2023-01-09-22\", \"2023-01-10-07\", \"2023-01-10-14\", \"2023-01-13-07\", \"2023-01-16-01\", \"2023-01-16-08\", \"2023-01-16-15\", \"2023-01-16-22\", \"2023-01-17-07\", \"2023-01-17-14\", \"2023-01-18-01\"]\n",
    "datasets_prj_rim = [\"2023-06-09-01\", \"2023-07-28-04\", \"2023-06-24-02\", \"2023-07-07-11\", \"2023-08-07-01\", \"2023-06-24-11\", \"2023-07-07-18\", \"2023-08-18-11\", \"2023-06-24-28\", \"2023-07-11-02\", \"2023-08-22-08\", \"2023-07-12-01\", \"2023-07-01-09\", \"2023-07-13-01\", \"2023-06-09-10\", \"2023-07-07-01\", \"2023-08-07-16\", \"2023-08-22-01\", \"2023-08-23-23\", \"2023-08-25-02\", \"2023-09-15-01\", \"2023-09-15-08\", \"2023-08-18-18\", \"2023-08-19-01\", \"2023-08-23-09\", \"2023-08-25-09\", \"2023-09-01-01\", \"2023-08-31-03\", \"2023-07-01-01\", \"2023-07-01-23\"]\n",
    "datasets_prj_aversion = [\"2023-03-30-01\", \"2023-06-29-01\", \"2023-06-29-13\", \"2023-07-14-08\", \"2023-07-14-14\", \"2023-07-27-01\", \"2023-08-08-07\", \"2023-08-14-01\", \"2023-08-16-01\", \"2023-08-21-01\", \"2023-09-07-01\", \"2023-09-14-01\", \"2023-08-15-01\", \"2023-10-05-01\", \"2023-06-23-08\", \"2023-12-11-01\", \"2023-06-21-01\"]\n",
    "datasets_prj_5ht = [\"2022-07-26-31\", \"2022-07-26-38\", \"2022-07-27-31\", \"2022-07-27-38\", \"2022-07-27-45\", \"2022-08-02-31\", \"2022-08-02-38\", \"2022-08-03-31\"]\n",
    "datasets_prj_starvation = [\"2023-05-25-08\", \"2023-05-26-08\", \"2023-06-05-10\", \"2023-06-05-17\", \"2023-07-24-27\", \"2023-09-27-14\", \"2023-05-25-01\", \"2023-05-26-01\", \"2023-07-24-12\", \"2023-07-24-20\", \"2023-09-12-01\", \"2023-09-19-01\", \"2023-09-29-19\", \"2023-10-09-01\", \"2023-09-13-02\"]\n",
    "\n",
    "datasets = datasets_prj_neuropal + datasets_prj_rim + datasets_prj_aversion + datasets_prj_5ht + datasets_prj_starvation\n",
    "print(len(set(datasets)) == len(datasets))\n",
    "\n",
    "datasets_val = ['2023-06-24-02', '2023-08-07-01', '2023-08-19-01', # RIM datasets\n",
    "                '2022-07-26-01', '2023-01-23-21', '2023-01-23-01', # NeuroPAL datasets\n",
    "                '2023-07-14-08', # Aversion datasets\n",
    "                '2022-08-02-31', # 5-HT datasets\n",
    "                '2023-07-24-27', '2023-07-24-20'] # Starvation datasets\n",
    "datasets_test = ['2023-08-22-01', '2023-07-07-18', '2023-07-01-23',  # RIM datasets\n",
    "                 '2023-01-06-01', '2023-01-10-07', '2023-01-17-07', # Neuropal datasets\n",
    "                 '2023-08-21-01', \"2023-06-23-08\", # Aversion datasets\n",
    "                 '2022-07-27-38', # 5-HT datasets\n",
    "                 '2023-10-09-01', '2023-09-13-02' # Starvation datasets\n",
    "                 ]\n",
    "datasets_train = [dataset for dataset in datasets if dataset not in datasets_val and dataset not in datasets_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download this file from [our Dropbox](https://www.dropbox.com/scl/fo/ealblchspq427pfmhtg7h/ALZ7AE5o3bT0VUQ8TTeR1As?rlkey=1e6tseyuwd04rbj7wmn2n6ij7&e=4&st=ybsvv0ry&dl=0) under `AutoCellLabeler/train_val_test_data/extracted_neuron_ids_final_1.h5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_extracted_neuron_ids = \"/store1/PublishedData/Data/prj_register/AutoCellLabeler/train_val_test_data/extracted_neuron_ids_final_1.h5\"\n",
    "extracted_neuron_ids = []\n",
    "with h5py.File(path_extracted_neuron_ids, 'r') as f:\n",
    "    extracted_neuron_ids = [name.decode('utf-8') for name in f['neuron_ids'][:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy files from freely-moving datasets\n",
    "\n",
    "This block of code copies random time points from freely-moving datasets, directly from ANTSUN output. There are also pre-copied files available in [our Dropbox](https://www.dropbox.com/scl/fo/ealblchspq427pfmhtg7h/ALZ7AE5o3bT0VUQ8TTeR1As?rlkey=1e6tseyuwd04rbj7wmn2n6ij7&e=4&st=ybsvv0ry&dl=0) under `AutoCellLabeler/freely_moving_eval`. If using those files, feel free to skip this section. If using ANTSUN output, it is important to run this code block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the paths to the data with where they are on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = {\n",
    "    \"prj_rim\": \"/store1/prj_rim/data_processed\",\n",
    "    \"prj_neuropal\": \"/store1/prj_neuropal/data_processed\",\n",
    "    \"prj_starvation\": \"/data1/prj_starvation/data_processed\",\n",
    "    \"prj_5ht\": \"/data3/prj_5ht/published_data/data_processed_neuropal\",\n",
    "    \"prj_aversion\": \"/data1/prj_aversion/data_processed\"\n",
    "}\n",
    "\n",
    "output_path_nrrd = \"/path/to/your/data_dir/nrrd\"\n",
    "output_path_roi = \"/path/to/your/data_dir/roi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_timepoints = {}\n",
    "n_trials = 100\n",
    "max_t = 1600 # max timepoint in each dataset\n",
    "\n",
    "for dataset in datasets_test:\n",
    "    if dataset in datasets_prj_rim:\n",
    "        prj_dir = input_paths[\"prj_rim\"]\n",
    "    elif dataset in datasets_prj_neuropal:\n",
    "        prj_dir = input_paths[\"prj_neuropal\"]\n",
    "    elif dataset in datasets_prj_starvation:\n",
    "        prj_dir = input_paths[\"prj_starvation\"]\n",
    "    elif dataset in datasets_prj_5ht:\n",
    "        prj_dir = input_paths[\"prj_5ht\"]\n",
    "    elif dataset in datasets_prj_aversion:\n",
    "        prj_dir = input_paths[\"prj_aversion\"]\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    base_path = os.path.join(prj_dir, dataset + \"_output\")\n",
    "    dataset_timepoints[dataset] = []\n",
    "    for trial in tqdm(range(n_trials)):\n",
    "        t = np.random.randint(max_t)\n",
    "        nrrd_path = os.path.join(base_path, \"NRRD_cropped\", dataset + \"_t\" + str(t).zfill(4) + \"_ch2.nrrd\")\n",
    "        watershed_path = os.path.join(base_path, \"img_roi_watershed\", str(t) + \".nrrd\")\n",
    "        n_reattempts = 0\n",
    "        while (t in dataset_timepoints[dataset] or not os.path.exists(nrrd_path) or not os.path.exists(watershed_path)) and n_reattempts < 1000:\n",
    "            t = np.random.randint(max_t)\n",
    "            nrrd_path = os.path.join(base_path, \"NRRD_cropped\", dataset + \"_t\" + str(t).zfill(4) + \"_ch2.nrrd\")\n",
    "            watershed_path = os.path.join(base_path, \"img_roi_watershed\", str(t) + \".nrrd\")\n",
    "            n_reattempts += 1\n",
    "        if n_reattempts >= 1000:\n",
    "            raise(ValueError(\"Could not find valid timepoint for dataset \" + dataset))\n",
    "        \n",
    "        expand_nrrd_dimension(nrrd_path, os.path.join(output_path_nrrd, dataset + \"_\" + str(t) + \".nrrd\"))\n",
    "        shutil.copy(watershed_path, output_path_roi)\n",
    "        dataset_timepoints[dataset].append(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert files to H5\n",
    "\n",
    "This section converts some previously-copied files to H5 format. If you have already downloaded the data files from the Dropbox link above, start here. If you're using copied data from this notebook, set `input_path_nrrd` and `input_path_roi` to be the previous code block's `output_nrrd` and `output_path_roi`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path_nrrd = \"/store1/PublishedData/Data/prj_register/AutoCellLabeler/freely_moving_eval/nrrd\"\n",
    "input_path_roi = \"/store1/PublishedData/Data/prj_register/AutoCellLabeler/freely_moving_eval/img_roi_watershed\"\n",
    "\n",
    "output_path_roi_crop = \"/path/to/your/data_dir/roi_crop\"\n",
    "output_path_h5 = \"/path/to/your/data_dir/h5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_to_dict(directory):\n",
    "    \"\"\"\n",
    "    Helper function to list which timepoints were selected in each NRRD directory.\n",
    "    \"\"\"\n",
    "    files = os.listdir(directory)\n",
    "    uid_dict = {}\n",
    "    for file in files:\n",
    "        if file.endswith(\".nrrd\"):\n",
    "            uid, timepoint = file.split(\"_\")\n",
    "            timepoint = timepoint.split(\".\")[0]  # Remove the extension\n",
    "            if uid in uid_dict:\n",
    "                uid_dict[uid].append(timepoint)\n",
    "            else:\n",
    "                uid_dict[uid] = [timepoint]\n",
    "    return uid_dict\n",
    "\n",
    "\n",
    "uid_timepoint_dict = list_files_to_dict(input_path_nrrd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(output_path_roi_crop, dataset + \"_\" + t + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropouts = {}\n",
    "\n",
    "for dataset in uid_timepoint_dict:\n",
    "    cropouts[dataset] = []\n",
    "    for t in tqdm(uid_timepoint_dict[dataset]):\n",
    "        cropout = create_h5_from_nrrd(\n",
    "                os.path.join(input_path_nrrd, dataset + \"_\" + t + \".nrrd\"),\n",
    "                os.path.join(output_path_h5, dataset + \"_\" + t + \".h5\"),\n",
    "                os.path.join(input_path_roi, dataset + \"_\" + t + \".nrrd\"),\n",
    "                os.path.join(output_path_roi_crop, dataset + \"_\" + t + \".h5\"),\n",
    "                (64, 120, 284), # crop size\n",
    "                len(extracted_neuron_ids)+1\n",
    "        )\n",
    "        cropouts[dataset].append(cropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run TagRFP-only AutoCellLabeler\n",
    "\n",
    "To run the TagRFP-only AutoCellLabeler network on these H5 files, see the [`pytorch-3dunet` package](https://github.com/flavell-lab/pytorch-3dunet), which contains the code and parameter files for this `all_red` network. This network's weights can be found in [our Dropbox](https://www.dropbox.com/scl/fo/ealblchspq427pfmhtg7h/ALZ7AE5o3bT0VUQ8TTeR1As?rlkey=1e6tseyuwd04rbj7wmn2n6ij7&e=4&st=ybsvv0ry&dl=0) under `AutoCellLabeler/model_weights/paper_all_red.pytorch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ROI matches\n",
    "\n",
    "Assuming you are using ANTSUN-processed data, you can run the `extract_roi_matches.ipynb` Julia notebook to extract the ROI matches dictionary from the ANTSUN outputs. This dictionary is matches the ROIs in the freely-moving data to those in the immobilized NeuroPAL images, which can be used to assess the quality of the AutoCellLabeler predictions on the freely-moving data.\n",
    "\n",
    "If you are using the data from our Dropbox, this dictionary is available in `AutoCellLabeler/freely_moving_eval/roi_match.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5_to_dict(filename):\n",
    "    data = {}\n",
    "    with h5py.File(filename, 'r') as file:\n",
    "        # Iterate over each dataset in the file\n",
    "        for key in file.keys():\n",
    "            # Each dataset is loaded as a numpy array\n",
    "            data[key] = file[key][:].transpose()\n",
    "    return data\n",
    "\n",
    "roi_matches = load_h5_to_dict(\"/store1/PublishedData/Data/prj_register/AutoCellLabeler/freely_moving_eval/roi_match.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load AutoCellLabeler predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_csv = \"/store1/adam/test/AutoCellLabeler/csv\"# \"/path/to/your/data_dir/csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dict_paper_all_red_fm = {}\n",
    "contaminated_neurons_paper_all_red_fm = {}\n",
    "output_dict_paper_all_red_fm = {}\n",
    "roi_sizes = {}\n",
    "for dataset_test in uid_timepoint_dict:\n",
    "    prob_dict_paper_all_red_fm[dataset_test] = {}\n",
    "    contaminated_neurons_paper_all_red_fm[dataset_test] = {}\n",
    "    roi_sizes[dataset_test] = {}\n",
    "    output_dict_paper_all_red_fm[dataset_test] = {}\n",
    "    for t in tqdm(uid_timepoint_dict[dataset_test]):\n",
    "        prob_dict_paper_all_red_fm[dataset_test][t], contaminated_neurons_paper_all_red_fm[dataset_test][t] = create_probability_dict(\n",
    "                os.path.join(output_path_roi_crop, dataset_test + \"_\" + str(t) + \".h5\"), \n",
    "                os.path.join(output_path_h5, dataset_test + \"_\" + str(t) + \"_predictions.h5\"),\n",
    "                contamination_threshold=0.75\n",
    "        )\n",
    "        roi_sizes[dataset_test][t] = get_roi_size(os.path.join(output_path_roi_crop, dataset_test + \"_\" + str(t) + \".h5\"))\n",
    "        output_dict_paper_all_red_fm[dataset_test][t] = output_label_file(\n",
    "                prob_dict_paper_all_red_fm[dataset_test][t],\n",
    "                contaminated_neurons_paper_all_red_fm[dataset_test][t],\n",
    "                roi_sizes[dataset_test][t], \n",
    "                path_extracted_neuron_ids,\n",
    "                os.path.join(input_path_roi, dataset_test + \"_\" + t + \".nrrd\"),\n",
    "                os.path.join(output_path_csv, dataset_test + \"_\" + str(t) + \".csv\"),\n",
    "                max_prob_decrease=0.0, \n",
    "                confidence_demote=-1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse AutoCellLabeler predictions\n",
    "\n",
    "In the freely-moving dataset, we can take advantage of multiple timepoints of data to construct more accurate predictions. This code accomplishes this by averaging the probability arrays for a neuron across all the timepoints where that neuron was linked to an ROI in that timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_roi_matches = {}\n",
    "max_t_np = 0\n",
    "max_roi_count = 339 # maximum number of ROIs in the immobilized dataset. You may need to increase this.\n",
    "\n",
    "for dataset in uid_timepoint_dict:\n",
    "    summed_roi_matches[dataset] = np.zeros((max_roi_count, len(extracted_neuron_ids)+1))\n",
    "    count = np.zeros(339)\n",
    "    for t_str in uid_timepoint_dict[dataset]:\n",
    "        t = int(t_str)\n",
    "        for roi in prob_dict_paper_all_red_fm[dataset][t_str]:\n",
    "            if roi_matches[dataset][t,roi] > 0:\n",
    "                t_np = int(roi_matches[dataset][t-1,roi-1]) # -1s are necessary Julia -> Python conversion\n",
    "                if t_np > max_t_np:\n",
    "                    max_t_np = t_np\n",
    "                count[t_np] += 1\n",
    "                summed_roi_matches[dataset][t_np,:] += prob_dict_paper_all_red_fm[dataset][t_str][roi]\n",
    "\n",
    "    summed_roi_matches[dataset] /= np.maximum(np.sum(summed_roi_matches[dataset], axis=1), 1e-100)[:, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"/path/to/your/data_dir/summed_roi_matches_paper_all_red_fm.h5\", 'w') as f:\n",
    "    for dataset in summed_roi_matches:\n",
    "        f.create_dataset(dataset, data=summed_roi_matches[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nested_dict_to_h5(filename, data):\n",
    "    with h5py.File(filename, 'w') as file:\n",
    "        for dataset, times in data.items():\n",
    "            dataset_group = file.create_group(dataset)\n",
    "            for timepoint, rois in times.items():\n",
    "                timepoint_group = dataset_group.create_group(str(timepoint))\n",
    "                for roi, array in rois.items():\n",
    "                    timepoint_group.create_dataset(str(roi), data=array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_nested_dict_to_h5('/data3/adam/new_unet_train/test_paper_noisy/prob_dict.h5', prob_dict_paper_all_red_fm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Julia)",
   "language": "python",
   "name": "julia_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
