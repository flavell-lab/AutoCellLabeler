{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate AutoCellLabeler on SWF415 Data\n",
    "\n",
    "This notebook evaluates the performance of the TagRFP-only AutoCellLabeler network on freely-moving data from the SWF415 strain, a different strain than the NeuroPAL strain the network was trained on.\n",
    "\n",
    "This notebook assumes that you have already run the `AutoCellLabeler_freely_moving` notebook through the \"Run TagRFP-only AutoCellLabeler\" section to format the SWF415 data and run the model on it.\n",
    "\n",
    "This notebook currently only supports using one time point per animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flavell lab packages\n",
    "using ND2Process\n",
    "using GPUFilter\n",
    "using WormFeatureDetector\n",
    "using NRRDIO\n",
    "using FlavellBase\n",
    "using SegmentationTools\n",
    "using ImageDataIO\n",
    "using RegistrationGraph\n",
    "using ExtractRegisteredData\n",
    "using CaAnalysis\n",
    "using BehaviorDataNIR\n",
    "using UNet2D\n",
    "\n",
    "\n",
    "# Other packages\n",
    "using ProgressMeter\n",
    "using PyCall\n",
    "using PyPlot\n",
    "using Statistics\n",
    "using StatsBase\n",
    "using DelimitedFiles\n",
    "using Images\n",
    "using Cairo\n",
    "using Distributions\n",
    "using DataStructures\n",
    "using HDF5\n",
    "using Interact\n",
    "using WebIO\n",
    "using Plots\n",
    "# using GraphPlot\n",
    "# using LightGraphs\n",
    "# using SimpleWeightedGraphs\n",
    "using Dates\n",
    "using JLD2\n",
    "using TotalVariation\n",
    "using VideoIO\n",
    "using Distributions\n",
    "using MultivariateStats\n",
    "using FFTW\n",
    "using LinearAlgebra\n",
    "using GLMNet\n",
    "using InformationMeasures\n",
    "using CUDA\n",
    "using LsqFit\n",
    "using Optim\n",
    "using Rotations\n",
    "using CoordinateTransformations\n",
    "using ImageTransformations\n",
    "using Interpolations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SWF415 Datasets\n",
    "\n",
    "This section loads the ANTSUN outputs for the SWF415 datasets so that we can access the mapping between ROIs at the timepoints where AutoCellLabeler was run and neuron indices used to fit CePNEM.\n",
    "\n",
    "This data is publicly available in [our Dropbox](https://www.dropbox.com/scl/fo/fb1cdxbwznhjp491ru6uq/ANJqOrenXBhA7lfxLYarRE0?rlkey=9ljwkyfhphumymgyzxfdf4c7s&st=49ev0ivz&dl=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_baseline = [\"2021-05-26-07\", \"2021-06-11-01\", \"2021-08-04-06\", \"2021-08-17-01\", \"2021-08-18-01\", \"2021-09-22-05\", \"2021-10-26-01\", \"2021-11-12-01\", \"2021-11-12-05\", \"2022-01-09-01\", \"2022-01-17-01\", \"2022-04-05-01\", \"2022-04-12-04\", \"2022-04-14-04\"]\n",
    "datasets_stim = [\"2021-09-06-09\", \"2021-09-14-01\", \"2021-09-14-05\", \"2021-09-23-01\", \"2021-09-30-01\"]\n",
    "datasets_stim_1600 = [\"2022-02-08-04\", \"2022-02-16-01\", \"2022-02-16-04\", \"2022-03-15-04\", \"2022-03-22-01\", \"2022-04-18-04\"]\n",
    "datasets_baseline_1600 = [\"2022-01-17-01\", \"2022-04-05-01\", \"2022-04-12-04\", \"2022-04-14-04\"]\n",
    "\n",
    "datasets = deepcopy(datasets_baseline)\n",
    "append!(datasets, datasets_stim)\n",
    "append!(datasets, datasets_stim_1600)\n",
    "\n",
    "length(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts = Dict()\n",
    "params_dict = Dict()\n",
    "param_paths = Dict()\n",
    "for dataset in datasets\n",
    "    path_root_process = \"/store1/prj_kfc/data_processed/$(dataset)_output\"\n",
    "    path_param_path = joinpath(path_root_process, \"param_path.jld2\")\n",
    "    \n",
    "    if isfile(path_param_path)\n",
    "        f = JLD2.jldopen(path_param_path)\n",
    "        param_paths[dataset] = f[\"param_path\"]\n",
    "        close(f)\n",
    "    else\n",
    "        @warn(\"No param_path.jld2 file found for dataset: $dataset\")\n",
    "    end\n",
    "    param_path = param_paths[dataset]\n",
    "\n",
    "    change_rootpath!(param_path, path_root_process)\n",
    "\n",
    "    if isfile(param_path[\"path_param\"])\n",
    "        f = JLD2.jldopen(param_path[\"path_param\"])\n",
    "        params_dict[dataset] = f[\"param\"]\n",
    "        close(f)\n",
    "    end\n",
    "    \n",
    "    param = params_dict[dataset]\n",
    "\n",
    "    add_get_basename!(param_path)\n",
    "    \n",
    "    if isfile(param_path[\"path_data_dict\"])\n",
    "        f = JLD2.jldopen(param_path[\"path_data_dict\"])\n",
    "        data_dicts[dataset] = f[\"data_dict\"]\n",
    "        close(f)\n",
    "    else\n",
    "        data_dicts[dataset] = Dict()\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select which timepoint to use\n",
    "\n",
    "By default, use the timepoint with the most neurons detected in the freely-moving traces. Note that you must also use this timepoint in the `AutoCellLabeler_freely_moving` notebook.\n",
    "\n",
    "If you used other timepoints in the `AutoCellLabeler_freely_moving` notebook, set `best_timepts` to those timepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_timepts = Dict()\n",
    "\n",
    "for dataset in datasets\n",
    "    taq = data_dicts[dataset][\"traces_array_quality\"]\n",
    "    # find column with most non-interpolated entries\n",
    "    best_timepts[dataset] = argmax(sum(taq .> 0.00001, dims=1)[1,:])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data\n",
    "\n",
    "After running `AutoCellLabeler_freely_moving` through the \"Run TagRFP-only AutoCellLabeler\" section, import the data here. Set `output_path` to the location of the `AutoCellLabeler_freely_moving` output. It should contain the following subdirectories:\n",
    "\n",
    "- `h5` should contain the AutoCellLabeler input and prediction files\n",
    "- `roi` should contain the original, uncropped ROI files from SegmentationNet\n",
    "- `roi_crop` should contain the same ROI files but cropped to AutoCellLabeler's crop size\n",
    "\n",
    "The fully-processed data for some SWF415 datasets is available on [our Dropbox](https://www.dropbox.com/scl/fo/ealblchspq427pfmhtg7h/ALZ7AE5o3bT0VUQ8TTeR1As?rlkey=1e6tseyuwd04rbj7wmn2n6ij7&st=ybsvv0ry&dl=0) under `AutoCellLabeler/SWF415_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/store1/PublishedData/Data/prj_register/AutoCellLabeler/SWF415_data\"\n",
    "output_path = \"/data3/adam/new_unet_train/swf415_test\"\n",
    "\n",
    "create_dir(joinpath(output_path, \"csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets_baseline\n",
    "    data_dict = data_dicts[dataset]\n",
    "    param = params_dict[dataset]\n",
    "    param_path = param_paths[dataset]\n",
    "    autolabel = pyimport(\"autolabel\")\n",
    "\n",
    "    param[\"autolabel_merged_rois_dist_thresh\"] = 8\n",
    "    param[\"autolabel_alt_label_threshold\"] = 0.0\n",
    "    param[\"autolabel_minimum_probability\"] = 0.01\n",
    "    param[\"autolabel_lrswap_threshold\"] = 0.1\n",
    "    param[\"autolabel_roi_edge_weight\"] = 0.01\n",
    "    param[\"autolabel_contamination_confidence_threshold\"] = 0.75\n",
    "    param[\"autolabel_contamination_num_threshold\"] = 10\n",
    "    param[\"autolabel_contamination_frac_threshold\"] = 0.2\n",
    "    param[\"autolabel_exclude_rois\"] = []\n",
    "    \n",
    "    data_dict[\"roi_sizes\"] = autolabel.get_roi_size(joinpath(output_path, \"roi_crop\", \"$(dataset).h5\"))\n",
    "    path_predictions = joinpath(output_path, \"h5\", \"$(dataset)_predictions.h5\")\n",
    "    data_dict[\"neuropal_probability_dict\"], data_dict[\"contaminated_neurons\"] = autolabel.create_probability_dict(joinpath(output_path, \"roi_crop\", \"$(dataset).h5\"), path_predictions)\n",
    "    data_dict[\"neuropal_label_data\"] = autolabel.output_label_file(data_dict[\"neuropal_probability_dict\"], data_dict[\"contaminated_neurons\"],\n",
    "            data_dict[\"roi_sizes\"], \"/data3/adam/new_unet_train/extracted_neuron_ids_final_1.h5\", joinpath(output_path, \"roi\", \"$(dataset).nrrd\"), joinpath(output_path, \"csv\", \"$(dataset).csv\"), \n",
    "            max_distance=param[\"autolabel_merged_rois_dist_thresh\"], max_prob_decrease=param[\"autolabel_alt_label_threshold\"], \n",
    "            min_prob=param[\"autolabel_minimum_probability\"], exclude_rois=param[\"autolabel_exclude_rois\"], \n",
    "            roi_matches=[],\n",
    "            lrswap_threshold=param[\"autolabel_lrswap_threshold\"], contamination_threshold=param[\"autolabel_contamination_num_threshold\"],\n",
    "            contamination_frac_threshold=param[\"autolabel_contamination_frac_threshold\"]\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CePNEM analysis dictionaries\n",
    "\n",
    "These dictionaries store the results of the CePNEM model fits for all datasets. In the absence of available human labels for the SWF415 strain, consistency of these fits is used to evaluate the performance of AutoCellLabeler. They are available in [our Dropbox](https://www.dropbox.com/scl/fo/gms8q0hzcufczrqlpsk95/ANrhFTrQaLN6XJrTzkk5rBA?rlkey=ciiyxvsd3ya6i7tsyttnikfed&st=75yhtvvy&dl=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_analysis_dict = \"/store1/PublishedData/Data/prj_neuropal/analysis_dict.jld2\"\n",
    "\n",
    "analysis_dict = Dict()\n",
    "if isfile(path_analysis_dict)\n",
    "    f = JLD2.jldopen(path_analysis_dict)\n",
    "    analysis_dict = f[\"analysis_dict\"]\n",
    "    close(f)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_classes = [\"glia\", \"granule\", \"RIFL\", \"RIFR\", \"AFDL\", \"AFDR\", \"RMFL\", \"RMFR\", \"SIADL\", \"SIADR\", \"VA01\", \"VD01\", \"AVG\", \"DD01\", \"SABVL\", \"SABVR\", \"SIBDL\", \"SIBDR\", \"ADFL\", \"RIGL\", \"RIGR\", \"AVFL\", \"DB02\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fit_results_lite = \"/store1/PublishedData/Data/prj_neuropal/fit_results_lite.jld2\"\n",
    "\n",
    "fit_results = Dict()\n",
    "if isfile(path_fit_results_lite)\n",
    "    f = JLD2.jldopen(path_fit_results_lite)\n",
    "    fit_results = f[\"fit_results_lite\"]\n",
    "    close(f)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_neuropal_stim = [\"2022-12-21-06\", \"2023-01-05-01\", \"2023-01-05-18\", \"2023-01-06-01\", \"2023-01-06-08\", \"2023-01-06-15\", \"2023-01-09-08\", \"2023-01-09-15\", \"2023-01-09-22\", \"2023-01-10-07\", \"2023-01-10-14\", \"2023-01-13-07\", \"2023-01-16-01\", \"2023-01-16-08\", \"2023-01-16-15\", \"2023-01-16-22\", \"2023-01-17-07\", \"2023-01-17-14\", \"2023-01-18-01\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine AutoCellLabeler concordance with CePNEM expectations\n",
    "\n",
    "In this code:\n",
    "\n",
    "- `neurons_rev` represents a list of neurons expected to encode reverse locomotion\n",
    "- `neurons_fwd` represents a list of neurons expected to encode forward locomotion\n",
    "- `neurons_dorsal` represents a list of neurons expected to encode dorsal locomotion\n",
    "- `neurons_ventral` represents a list of neurons expected to encode ventral locomotion\n",
    "\n",
    "The code computes the fraction of times the AutoCellLabeler labels for those neurons encode the expected locomotion property, and compares it against the fraction of human-labeled neurons (in the NeuroPAL strain) that encode the expected locomotion property. Specifically, the code simulates `n_trials` of randomly sampling each labeled neuron as either a random human label for that neuron (ie: AutoCellLabeler got the label right), or a random other neuron in that dataset (ie: AutoCellLabeler got the label wrong). In this way we can account for either (i) CePNEM failing to find significant encoding even when the label was correct or (ii) the neuron getting mislabeled as another neuron with the correct encoding.\n",
    "\n",
    "By default, only baseline datasets are used since the heat stimulation can disrupt neural encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_rev = [\"AVAL\", \"AVAR\", \"AVA?\", \"AVEL\", \"AVER\", \"AVE?\", \"RIML\", \"RIMR\", \"RIM?\", \"AIBL\", \"AIBR\", \"AIB?\"]\n",
    "neurons_fwd = [\"RIBL\", \"RIBR\", \"RIB?\", \"AVBL\", \"AVBR\", \"AVB?\", \"RID\", \"RMEL\", \"RMER\", \"RMED\"]\n",
    "neurons_dorsal = [\"SMDDL\", \"SMDDR\"]\n",
    "neurons_ventral = [\"SMDVL\", \"SMDVR\", \"RIVL\", \"RIVR\"]\n",
    "\n",
    "neurons_nolr = [\"RID\", \"RMED\"]\n",
    "\n",
    "beh_dict = Dict(\n",
    "    \"rev\" => \"v\",\n",
    "    \"fwd\" => \"v\",\n",
    "    \"dorsal\" => \"θh\",\n",
    "    \"ventral\" => \"θh\"\n",
    ")\n",
    "\n",
    "swap_dict = Dict(\n",
    "    \"rev\" => \"fwd\",\n",
    "    \"fwd\" => \"rev\",\n",
    "    \"dorsal\" => \"ventral\",\n",
    "    \"ventral\" => \"dorsal\"\n",
    ")\n",
    "\n",
    "neurons_of_interest = deepcopy(neurons_rev)\n",
    "append!(neurons_of_interest, neurons_fwd)\n",
    "append!(neurons_of_interest, neurons_dorsal)\n",
    "append!(neurons_of_interest, neurons_ventral)\n",
    "conf_thresh = 3\n",
    "\n",
    "n_correct = 0\n",
    "n_incorrect = 0\n",
    "n_unknown = 0\n",
    "n_both = 0\n",
    "n_notrace = 0\n",
    "conf = []\n",
    "\n",
    "acc_fracs = 0:100\n",
    "n_trials = 1000\n",
    "n_correct_randomcontrol = zeros(length(acc_fracs), n_trials)\n",
    "n_incorrect_randomcontrol = zeros(length(acc_fracs), n_trials)\n",
    "n_unknown_randomcontrol = zeros(length(acc_fracs), n_trials)\n",
    "n_both_randomcontrol = zeros(length(acc_fracs), n_trials)\n",
    "\n",
    "all_correct = []\n",
    "all_incorrect = []\n",
    "all_unknown = []\n",
    "all_both = []\n",
    "conf_succeed = Dict()\n",
    "\n",
    "@showprogress for dataset in datasets_baseline\n",
    "    data_dict = data_dicts[dataset]\n",
    "    conf_succeed[dataset] = []\n",
    "\n",
    "    inv_valid_rois = Dict()\n",
    "    for (i,roi) in enumerate(data_dict[\"valid_rois\"])\n",
    "        inv_valid_rois[roi] = UInt16(i)\n",
    "    end\n",
    "    for data in data_dict[\"neuropal_label_data\"]\n",
    "\n",
    "        if data[\"confidence\"] >= conf_thresh && !(data[\"neuron_class\"] in excluded_classes) && !occursin(\"alt\", data[\"neuron_class\"])\n",
    "            push!(conf_succeed[dataset], (data[\"neuron_class\"], data[\"max_prob\"]))\n",
    "        else\n",
    "            continue\n",
    "        end\n",
    "        initial_neuron_idx = get(inv_valid_rois, get(data_dict[\"new_label_map\"][best_timepts[dataset]], Int(data[\"roi_id\"]), nothing), nothing)\n",
    "        neuron_idx = nothing\n",
    "        found = false\n",
    "        for k in keys(data_dicts[dataset])\n",
    "            if occursin(\"successful_idx_\", k)\n",
    "                found = true\n",
    "                for (i, n) in enumerate(data_dicts[dataset][k])\n",
    "                    if n == initial_neuron_idx\n",
    "                        neuron_idx = i\n",
    "                        break\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if !found\n",
    "            neuron_idx = initial_neuron_idx\n",
    "        end\n",
    "\n",
    "        # neuron_idx = rand(1:size(data_dicts[dataset][\"traces_array\"], 1))\n",
    "\n",
    "        if data[\"neuron_class\"] in neurons_of_interest && data[\"confidence\"] >= conf_thresh\n",
    "            push!(conf, data[\"max_prob\"])\n",
    "            if isnothing(neuron_idx)\n",
    "                n_notrace += 1\n",
    "                continue\n",
    "            end\n",
    "            encoding = nothing\n",
    "            if data[\"neuron_class\"] in neurons_rev\n",
    "                encoding = \"rev\"\n",
    "            elseif data[\"neuron_class\"] in neurons_fwd\n",
    "                encoding = \"fwd\"\n",
    "            elseif data[\"neuron_class\"] in neurons_dorsal\n",
    "                encoding = \"dorsal\"\n",
    "            elseif data[\"neuron_class\"] in neurons_ventral\n",
    "                encoding = \"ventral\"\n",
    "            else\n",
    "                error(\"Unknown neuron class: $(data[\"neuron_class\"])\")\n",
    "            end\n",
    "            correct = false\n",
    "            incorrect = false\n",
    "            beh = beh_dict[encoding]\n",
    "            swap = swap_dict[encoding]\n",
    "            for rng in analysis_dict[\"enc_change_rngs\"][dataset]\n",
    "                if neuron_idx in analysis_dict[\"neuron_categorization\"][dataset][rng][beh][encoding]\n",
    "                    correct = true\n",
    "                end\n",
    "                if neuron_idx in analysis_dict[\"neuron_categorization\"][dataset][rng][beh][swap]\n",
    "                    incorrect = true\n",
    "                end\n",
    "            end\n",
    "            if correct\n",
    "                if incorrect\n",
    "                    n_both += 1\n",
    "                    push!(all_both, (dataset, Int(neuron_idx), data[\"neuron_class\"], data[\"max_prob\"]))\n",
    "                else\n",
    "\n",
    "                    n_correct += 1\n",
    "                    push!(all_correct, (dataset, Int(neuron_idx), data[\"neuron_class\"], data[\"max_prob\"]))\n",
    "                end\n",
    "            elseif incorrect\n",
    "                n_incorrect += 1\n",
    "                push!(all_incorrect, (dataset, Int(neuron_idx), data[\"neuron_class\"], data[\"max_prob\"]))\n",
    "            else\n",
    "                push!(all_unknown, (dataset, Int(neuron_idx), data[\"neuron_class\"], data[\"max_prob\"]))\n",
    "                n_unknown += 1\n",
    "            end\n",
    "\n",
    "            neuron_class_nolr = data[\"neuron_class\"]\n",
    "            if !(data[\"neuron_class\"] in neurons_nolr)\n",
    "                neuron_class_nolr = neuron_class_nolr[1:end-1]\n",
    "            end\n",
    "            for i in 1:n_trials\n",
    "                for (j, acc) in enumerate(acc_fracs)\n",
    "                    # use legitimate encoding\n",
    "                    if rand() < acc / 100\n",
    "                        (dataset_rand, n_rand) = rand(analysis_dict[\"matches\"][neuron_class_nolr])\n",
    "                        count = 0\n",
    "                        # exclude heat-stim datasets\n",
    "                        while dataset_rand in datasets_neuropal_stim\n",
    "                            (dataset_rand, n_rand) = rand(analysis_dict[\"matches\"][neuron_class_nolr])\n",
    "                            count += 1\n",
    "                            # avoid infinite loops\n",
    "                            if count > 10000\n",
    "                                @warn(\"Could not find neuron $(neuron_class_nolr) in baseline data.\")\n",
    "                                break\n",
    "                            end\n",
    "                        end\n",
    "                        \n",
    "                        correct_random = false\n",
    "                        incorrect_random = false\n",
    "                        for rng in analysis_dict[\"enc_change_rngs\"][dataset_rand]\n",
    "                            if n_rand in analysis_dict[\"neuron_categorization\"][dataset_rand][rng][beh][encoding]\n",
    "                                correct_random = true\n",
    "                            end\n",
    "                            if n_rand in analysis_dict[\"neuron_categorization\"][dataset_rand][rng][beh][swap]\n",
    "                                incorrect_random = true\n",
    "                            end\n",
    "                        end\n",
    "                        if correct_random\n",
    "                            if incorrect_random\n",
    "                                n_both_randomcontrol[j,i] += 1\n",
    "                            else\n",
    "                                n_correct_randomcontrol[j,i] += 1\n",
    "                            end\n",
    "                        elseif incorrect_random\n",
    "                            n_incorrect_randomcontrol[j,i] += 1\n",
    "                        else\n",
    "                            n_unknown_randomcontrol[j,i] += 1\n",
    "                        end\n",
    "                    else\n",
    "                        neuron_idx_random = rand(1:size(data_dicts[dataset][\"traces_array\"], 1))\n",
    "                        correct_random = false\n",
    "                        incorrect_random = false\n",
    "                        for rng in analysis_dict[\"enc_change_rngs\"][dataset]\n",
    "                            if neuron_idx_random in analysis_dict[\"neuron_categorization\"][dataset][rng][beh][encoding]\n",
    "                                correct_random = true\n",
    "                            end\n",
    "                            if neuron_idx_random in analysis_dict[\"neuron_categorization\"][dataset][rng][beh][swap]\n",
    "                                incorrect_random = true\n",
    "                            end\n",
    "                        end\n",
    "                        if correct_random\n",
    "                            if incorrect_random\n",
    "                                n_both_randomcontrol[j,i] += 1\n",
    "                            else\n",
    "                                n_correct_randomcontrol[j,i] += 1\n",
    "                            end\n",
    "                        elseif incorrect_random\n",
    "                            n_incorrect_randomcontrol[j,i] += 1\n",
    "                        else\n",
    "                            n_unknown_randomcontrol[j,i] += 1\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "println(n_correct, \" \", n_incorrect, \" \", n_unknown, \" \", n_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw fraction of labels with expected encoding (not corrected for random sample control):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct / (n_correct + n_incorrect + n_unknown + n_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results\n",
    "\n",
    "Display the confidence interval based on the simulations, and plot the simulated vs observed CePNEM concordance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    acc = n_correct_randomcontrol ./ (n_correct_randomcontrol .+ n_incorrect_randomcontrol .+ n_unknown_randomcontrol .+ n_both_randomcontrol)\n",
    "    \n",
    "    # Compute mean accuracy and 95th percentile error bounds\n",
    "    mean_acc = [mean(acc[i, :]) for i in 1:101]\n",
    "    lower_bound = [quantile(acc[i, :], 0.025) for i in 1:101]\n",
    "    upper_bound = [quantile(acc[i, :], 0.975) for i in 1:101]\n",
    "\n",
    "    # Measured values for the parameter\n",
    "    measured_values = [n_correct / (n_correct + n_incorrect + n_unknown + n_both) for i in 1:101]\n",
    "\n",
    "\n",
    "    # Find the first and last value of the parameter within the 95% confidence interval\n",
    "    inside_interval = [(measured_values[i] >= lower_bound[i] && measured_values[i] <= upper_bound[i]) for i in 1:101]\n",
    "    first_inside = findfirst(inside_interval)\n",
    "    last_inside = findlast(inside_interval)\n",
    "\n",
    "    # Find the value in control data closest to the measured value for each parameter\n",
    "    closest_values = [mean(acc[i, :]) for i in 1:101]\n",
    "  \n",
    "    \n",
    "    println(\"First parameter where measured value falls inside the 95% CI: \", first_inside - 1)\n",
    "    println(\"Last parameter where measured value falls inside the 95% CI: \", last_inside - 1)\n",
    "    println(\"Closest parameter to measured value: \", argmin(abs.(closest_values .- measured_values)) - 1)\n",
    "    \n",
    "    # Generate x values (parameters)\n",
    "    x = (0:100) ./ 100\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = subplots(figsize=(4.5,2))\n",
    "\n",
    "    # Plot mean accuracy with shaded 95th percentile error bounds\n",
    "    ax.plot(x, mean_acc, label=\"Simulations\", linewidth=2)\n",
    "    ax.fill_between(x, lower_bound, upper_bound, alpha=0.2)\n",
    "\n",
    "    \n",
    "    intersection_index = argmin(abs.(mean_acc .- measured_values))\n",
    "    intersection_value = x[intersection_index]\n",
    "    intersection_y = mean_acc[intersection_index]\n",
    "\n",
    "    # Add a dashed vertical line at the intersection point\n",
    "    ax.plot([intersection_value, intersection_value], [0, intersection_y], \"k--\", linewidth=2, label=nothing)\n",
    "\n",
    "    # Add the second plot\n",
    "    ax.scatter([intersection_value], [measured_values[1]], label=\"Autolabel\", s=25, zorder=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Highlight the first and last inside interval points\n",
    "    # if first_inside !== nothing\n",
    "    #     ax.plot(x[first_inside], measured_values[first_inside], \"go\", label=\"First Inside Interval\")\n",
    "    # end\n",
    "    # if last_inside !== nothing\n",
    "    #     ax.plot(x[last_inside], measured_values[last_inside], \"ro\", label=\"Last Inside Interval\")\n",
    "    # end\n",
    "\n",
    "    # Customize plot\n",
    "    # ax.set_xlabel(\"Parameter\", fontsize=7, fontname=\"DejaVu Sans\")\n",
    "    # ax.set_ylabel(\"Accuracy\", fontsize=7, fontname=\"DejaVu Sans\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.legend(loc=\"lower right\", fontsize=7)\n",
    "\n",
    "    # Customize ticks\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=7)\n",
    "\n",
    "    # Add bolded x tick mark at the intersection point\n",
    "    ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0, intersection_value])\n",
    "    ax.get_xticklabels()[end].set_weight(\"bold\")\n",
    "\n",
    "    # Disable top and right axes\n",
    "    ax.spines[\"top\"].set_visible(false)\n",
    "    ax.spines[\"right\"].set_visible(false)\n",
    "\n",
    "    # Display the plot\n",
    "    show()\n",
    "    # PyPlot.savefig(\"/data3/prj_register/figures/figure_5/swf415_accuracy.pdf\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
